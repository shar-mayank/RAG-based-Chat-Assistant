import os
import time
import hashlib
import pickle
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional, Generator
from dataclasses import dataclass
from datetime import datetime
import re
import fitz  # PyMuPDF - more robust PDF processing

# OCR dependencies
import pytesseract
from pdf2image import convert_from_path
from PIL import Image
import tempfile

# Core dependencies
from langchain_ollama import OllamaLLM, OllamaEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda, RunnableParallel
from langchain_core.documents import Document

# File monitoring
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

@dataclass
class RAGConfig:
  """Configuration for the RAG system"""
  pdf_folder: str = "./documents"
  vector_db_path: str = "./vector_db"
  model_name: str = "gemma3:4b"
  embedding_model: str = "bge-m3:latest"
  chunk_size: int = 1000
  chunk_overlap: int = 200
  max_retrieval_docs: int = 4
  temperature: float = 0.0
  top_p: float = 0.1
  debug_mode: bool = False  # Add debug mode flag
  ocr_dpi: int = 300  # DPI for OCR image conversion
  ocr_lang: str = 'eng'  # OCR language

class EnhancedDocumentProcessor:
  """Enhanced PDF processing with multiple extraction methods"""

  def __init__(self, config: RAGConfig):
    self.config = config
    self.text_splitter = RecursiveCharacterTextSplitter(
      chunk_size=config.chunk_size,
      chunk_overlap=config.chunk_overlap,
      length_function=len,
      separators=["\n\n", "\n", " ", ""]
    )

    # Check OCR dependencies
    self._check_ocr_dependencies()

  def _check_ocr_dependencies(self):
    """Check if OCR dependencies are available"""
    try:
      # Test pytesseract
      pytesseract.get_tesseract_version()
      self.ocr_available = True
      print("OCR dependencies available")
    except Exception as e:
      self.ocr_available = False
      print(f"OCR not available: {str(e)}")
      print("   Install with: pip install pytesseract pdf2image pillow")
      print("   Also install Tesseract OCR: https://github.com/tesseract-ocr/tesseract")

  def _debug_print_extraction(self, method: str, text: str, page_num: int = None):
    """Debug function to print extracted text"""
    if not self.config.debug_mode:
      return

    print(f"\n{'='*60}")
    print(f"DEBUG: {method} Extraction")
    if page_num is not None:
      print(f"Page: {page_num}")
    print(f"{'='*60}")

    # Print first 500 characters
    preview = text[:500] if text else "No text extracted"
    print(f"Text preview: {preview}")

    if text:
      print(f"Total length: {len(text)} characters")
      print(f"Lines: {text.count(chr(10)) + 1}")
      print(f"Words (approx): {len(text.split())}")

    print(f"{'='*60}\n")

  def process_pdf(self, pdf_path: str) -> List[Document]:
    """Process a single PDF file with multiple extraction methods"""
    try:
      print(f"Processing {os.path.basename(pdf_path)}...")

      # Method 1: Try PyMuPDF first (most robust)
      documents = self._extract_with_pymupdf(pdf_path)

      # Method 2: Fallback to PyPDF if PyMuPDF fails
      if not documents:
        print("PyMuPDF failed, trying PyPDF...")
        documents = self._extract_with_pypdf(pdf_path)

      # Method 3: Last resort - try to extract images with OCR
      if not documents and self.ocr_available:
        print("PyPDF failed, trying OCR extraction...")
        documents = self._extract_with_ocr(pdf_path)

      if not documents:
        print(f"Failed to extract any text from {pdf_path}")
        return []

      # Add metadata
      for doc in documents:
        doc.metadata.update({
          'source_file': os.path.basename(pdf_path),
          'processed_at': datetime.now().isoformat(),
          'file_hash': self._get_file_hash(pdf_path)
        })

      # Split documents
      split_docs = self._split_documents(documents)
      print(f"Extracted {len(split_docs)} chunks from {len(documents)} pages")

      return split_docs

    except Exception as e:
      print(f"Error processing {pdf_path}: {str(e)}")
      logging.error(f"Error processing {pdf_path}: {str(e)}")
      return []

  def _extract_with_pymupdf(self, pdf_path: str) -> List[Document]:
    """Extract text using PyMuPDF (most robust)"""
    try:
      doc = fitz.open(pdf_path)
      documents = []

      print(f"PyMuPDF: Processing {len(doc)} pages...")

      for page_num in range(len(doc)):
        page = doc[page_num]

        # Get text
        text = page.get_text()

        # Debug print
        self._debug_print_extraction("PyMuPDF", text, page_num + 1)

        # Get additional info for debugging
        if self.config.debug_mode:
          print(f"    Page {page_num + 1} info:")
          print(f"      - Text blocks: {len(page.get_text_blocks())}")
          print(f"      - Images: {len(page.get_images())}")
          print(f"      - Links: {len(page.get_links())}")

          # Try to get text with different methods
          text_dict = page.get_text("dict")
          print(f"      - Dict blocks: {len(text_dict.get('blocks', []))}")

        if text.strip():  # Only add pages with actual text
          documents.append(Document(
            page_content=text,
            metadata={
              'page': page_num + 1,
              'extraction_method': 'pymupdf',
              'char_count': len(text),
              'word_count': len(text.split())
            }
          ))
        else:
          print(f"    Page {page_num + 1}: No text found")

      doc.close()
      print(f"PyMuPDF: Extracted text from {len(documents)} pages")
      return documents

    except Exception as e:
      print(f"PyMuPDF extraction failed: {str(e)}")
      logging.error(f"PyMuPDF extraction failed: {str(e)}")
      return []

  def _extract_with_pypdf(self, pdf_path: str) -> List[Document]:
    """Fallback extraction using PyPDF"""
    try:
      print(f"PyPDF: Loading document...")
      loader = PyPDFLoader(pdf_path)
      documents = loader.load()

      print(f"PyPDF: Processing {len(documents)} pages...")

      processed_docs = []
      for i, doc in enumerate(documents):
        # Debug print
        self._debug_print_extraction("PyPDF", doc.page_content, i + 1)

        if self.config.debug_mode:
          print(f"    Page {i + 1} info:")
          print(f"      - Metadata: {doc.metadata}")
          print(f"      - Content length: {len(doc.page_content)}")
          print(f"      - Word count: {len(doc.page_content.split())}")

        if doc.page_content.strip():
          # Add extraction method to metadata
          doc.metadata.update({
            'extraction_method': 'pypdf',
            'char_count': len(doc.page_content),
            'word_count': len(doc.page_content.split())
          })
          processed_docs.append(doc)
        else:
          print(f"    Page {i + 1}: No text found")

      print(f"PyPDF: Extracted text from {len(processed_docs)} pages")
      return processed_docs

    except Exception as e:
      print(f"PyPDF extraction failed: {str(e)}")
      logging.error(f"PyPDF extraction failed: {str(e)}")
      return []

  def _extract_with_ocr(self, pdf_path: str) -> List[Document]:
    """OCR extraction using pytesseract and pdf2image"""
    if not self.ocr_available:
      print("OCR not available - skipping OCR extraction")
      return []

    try:
      print(f"OCR: Converting PDF to images (DPI: {self.config.ocr_dpi})...")

      # Convert PDF to images
      with tempfile.TemporaryDirectory() as temp_dir:
        try:
          images = convert_from_path(
            pdf_path,
            dpi=self.config.ocr_dpi,
            output_folder=temp_dir,
            first_page=None,
            last_page=None,
            fmt='png'
          )

          print(f"OCR: Converted to {len(images)} images")

        except Exception as e:
          print(f"OCR: PDF to image conversion failed: {str(e)}")
          return []

        documents = []

        # Process each image
        for page_num, image in enumerate(images):
          try:
            print(f"OCR: Processing page {page_num + 1}/{len(images)}...")

            # Configure OCR
            custom_config = f'--oem 3 --psm 6 -l {self.config.ocr_lang}'

            # Extract text
            text = pytesseract.image_to_string(image, config=custom_config)

            # Debug print
            self._debug_print_extraction("OCR", text, page_num + 1)

            if self.config.debug_mode:
              print(f"    OCR Page {page_num + 1} info:")
              print(f"      - Image size: {image.size}")
              print(f"      - Image mode: {image.mode}")
              print(f"      - Text length: {len(text)}")
              print(f"      - Word count: {len(text.split())}")

              # Get OCR confidence data
              try:
                data = pytesseract.image_to_data(image, output_type=pytesseract.Output.DICT)
                confidences = [int(conf) for conf in data['conf'] if int(conf) > 0]
                if confidences:
                  avg_confidence = sum(confidences) / len(confidences)
                  print(f"      - Average OCR confidence: {avg_confidence:.1f}%")
              except Exception as conf_e:
                print(f"      - Could not get confidence data: {str(conf_e)}")

            # Clean up OCR text
            text = self._clean_ocr_text(text)

            if text.strip():
              documents.append(Document(
                page_content=text,
                metadata={
                  'page': page_num + 1,
                  'extraction_method': 'ocr',
                  'char_count': len(text),
                  'word_count': len(text.split()),
                  'ocr_dpi': self.config.ocr_dpi,
                  'ocr_lang': self.config.ocr_lang
                }
              ))
            else:
              print(f"    OCR Page {page_num + 1}: No text found")

          except Exception as e:
            print(f"    OCR failed on page {page_num + 1}: {str(e)}")
            continue

        print(f"OCR: Extracted text from {len(documents)} pages")
        return documents

    except Exception as e:
      print(f"OCR extraction failed: {str(e)}")
      logging.error(f"OCR extraction failed: {str(e)}")
      return []

  def _clean_ocr_text(self, text: str) -> str:
    """Clean OCR text to remove common artifacts"""
    if not text:
      return ""

    # Remove excessive whitespace
    text = re.sub(r'\s+', ' ', text)

    # Remove isolated single characters (common OCR artifacts)
    text = re.sub(r'\s[a-zA-Z]\s', ' ', text)

    # Remove lines with only special characters
    lines = text.split('\n')
    cleaned_lines = []
    for line in lines:
      # Keep line if it has at least 3 alphanumeric characters
      if len(re.findall(r'[a-zA-Z0-9]', line)) >= 3:
        cleaned_lines.append(line.strip())

    return '\n'.join(cleaned_lines).strip()

  def _split_documents(self, documents: List[Document]) -> List[Document]:
    """Split documents into chunks"""
    if not documents:
      return []

    # Filter out very short documents
    filtered_docs = [doc for doc in documents if len(doc.page_content.strip()) > 50]

    if not filtered_docs:
      print("All pages too short after filtering")
      return []

    split_docs = self.text_splitter.split_documents(filtered_docs)

    if self.config.debug_mode:
      print(f"Document splitting summary:")
      print(f"    - Original pages: {len(documents)}")
      print(f"    - After filtering: {len(filtered_docs)}")
      print(f"    - Final chunks: {len(split_docs)}")

      # Show chunk size distribution
      chunk_sizes = [len(doc.page_content) for doc in split_docs]
      if chunk_sizes:
        print(f"    - Chunk sizes: min={min(chunk_sizes)}, max={max(chunk_sizes)}, avg={sum(chunk_sizes)/len(chunk_sizes):.0f}")

    return split_docs

  def _get_file_hash(self, filepath: str) -> str:
    """Generate hash for file to detect changes"""
    hash_md5 = hashlib.md5()
    with open(filepath, "rb") as f:
      for chunk in iter(lambda: f.read(4096), b""):
        hash_md5.update(chunk)
    return hash_md5.hexdigest()

class VectorStoreManager:
  """Manages vector database operations"""

  def __init__(self, config: RAGConfig):
    self.config = config
    self.embeddings = OllamaEmbeddings(model=config.embedding_model)
    self.vector_store = None
    self.processed_files = {}
    self._load_processed_files()

  def _load_processed_files(self):
    """Load record of processed files"""
    processed_files_path = os.path.join(self.config.vector_db_path, "processed_files.pkl")
    if os.path.exists(processed_files_path):
      with open(processed_files_path, 'rb') as f:
        self.processed_files = pickle.load(f)

  def _save_processed_files(self):
    """Save record of processed files"""
    os.makedirs(self.config.vector_db_path, exist_ok=True)
    processed_files_path = os.path.join(self.config.vector_db_path, "processed_files.pkl")
    with open(processed_files_path, 'wb') as f:
      pickle.dump(self.processed_files, f)

  def load_vector_store(self):
    """Load existing vector store"""
    vector_store_path = os.path.join(self.config.vector_db_path, "faiss_index")
    if os.path.exists(vector_store_path):
      try:
        self.vector_store = FAISS.load_local(
          vector_store_path,
          self.embeddings,
          allow_dangerous_deserialization=True
        )
        print("Loaded existing vector store")
        logging.info("Loaded existing vector store")
      except Exception as e:
        print(f"Error loading vector store: {str(e)}")
        logging.error(f"Error loading vector store: {str(e)}")
        self.vector_store = None

  def save_vector_store(self):
    """Save vector store to disk"""
    if self.vector_store:
      os.makedirs(self.config.vector_db_path, exist_ok=True)
      vector_store_path = os.path.join(self.config.vector_db_path, "faiss_index")
      self.vector_store.save_local(vector_store_path)
      self._save_processed_files()
      print("Vector store saved")
      logging.info("Saved vector store")

  def add_documents(self, documents: List[Document], file_path: str, file_hash: str):
    """Add documents to vector store"""
    if not documents:
      return

    try:
      print(f"Adding {len(documents)} chunks to vector store...")

      if self.vector_store is None:
        print("Creating new vector store...")
        self.vector_store = FAISS.from_documents(documents, self.embeddings)
      else:
        print("Adding to existing vector store...")
        self.vector_store.add_documents(documents)

      # Record the processed file with extraction method info
      extraction_methods = list(set([doc.metadata.get('extraction_method', 'unknown') for doc in documents]))

      self.processed_files[file_path] = {
        'hash': file_hash,
        'processed_at': datetime.now().isoformat(),
        'num_chunks': len(documents),
        'extraction_methods': extraction_methods
      }

      self.save_vector_store()
      print(f"Successfully added {len(documents)} chunks from {os.path.basename(file_path)}")
      print(f"   Extraction methods used: {', '.join(extraction_methods)}")
      logging.info(f"Added {len(documents)} documents from {file_path}")

    except Exception as e:
      print(f"Error adding documents: {str(e)}")
      logging.error(f"Error adding documents: {str(e)}")

  def get_retriever(self):
    """Get retriever for similarity search"""
    if self.vector_store:
      return self.vector_store.as_retriever(
        search_kwargs={"k": self.config.max_retrieval_docs}
      )
    return None

  def needs_processing(self, file_path: str, file_hash: str) -> bool:
    """Check if file needs processing"""
    if file_path not in self.processed_files:
      return True
    return self.processed_files[file_path]['hash'] != file_hash

class ResponseFilter:
    """Simplified response filter"""

    @staticmethod
    def clean_response(response: str) -> str:
        """Simple cleaning without aggressive filtering"""
        response = response.strip()

        if not response:
            return "I don't have enough information to answer this question."

        # Only remove obvious redundant prefixes
        prefixes_to_remove = [
            "According to the context, ",
            "Based on the context, ",
            "The context states that ",
            "From the context, ",
            "The document mentions that ",
            "As stated in the context, "
        ]

        response_lower = response.lower()
        for prefix in prefixes_to_remove:
            if response_lower.startswith(prefix.lower()):
                response = response[len(prefix):].strip()
                break

        return response

class PDFFileHandler(FileSystemEventHandler):
  """Handles file system events for PDF monitoring"""

  def __init__(self, rag_system):
    self.rag_system = rag_system

  def on_created(self, event):
    if not event.is_directory and event.src_path.endswith('.pdf'):
      print(f"New PDF detected: {os.path.basename(event.src_path)}")
      logging.info(f"New PDF detected: {event.src_path}")
      time.sleep(1)  # Wait for file to be completely written
      self.rag_system.process_single_file(event.src_path)

  def on_modified(self, event):
    if not event.is_directory and event.src_path.endswith('.pdf'):
      print(f"PDF modified: {os.path.basename(event.src_path)}")
      logging.info(f"PDF modified: {event.src_path}")
      time.sleep(1)  # Wait for file to be completely written
      self.rag_system.process_single_file(event.src_path)

class RobustRAGSystem:
  """Main RAG system class with streaming support"""

  def __init__(self, config: RAGConfig = None):
    self.config = config or RAGConfig()
    self.setup_logging()
    self.setup_directories()
    # Add conversation cache for semantic similarity
    self.conversation_cache = []
    self.cache_similarity_threshold = 0.85

    print("Initializing Robust RAG System...")
    if self.config.debug_mode:
      print("Debug mode enabled - detailed extraction info will be shown")

    # Initialize components
    self.doc_processor = EnhancedDocumentProcessor(self.config)
    self.vector_manager = VectorStoreManager(self.config)

    # Initialize model
    print("Initializing Ollama model...")
    self.model = OllamaLLM(
      model=self.config.model_name,
      temperature=self.config.temperature,
      top_p=self.config.top_p,
      repeat_penalty=1.1,
      num_predict=512,
    )

    # File monitoring
    self.observer = None

    # Load existing vector store
    self.vector_manager.load_vector_store()

    # Process existing files
    self.process_existing_files()

    # Setup query chain
    self.setup_query_chain()

    print("RAG System initialized successfully!")

  def setup_logging(self):
    """Setup logging configuration"""
    logging.basicConfig(
      level=logging.INFO,
      format='%(asctime)s - %(levelname)s - %(message)s',
      handlers=[
        logging.FileHandler('rag_system.log'),
        logging.StreamHandler()
      ]
    )

  def setup_directories(self):
    """Create necessary directories"""
    os.makedirs(self.config.pdf_folder, exist_ok=True)
    os.makedirs(self.config.vector_db_path, exist_ok=True)

  def setup_query_chain(self):
        """Setup the query processing chain with improved prompt"""

        # ULTRA-SIMPLE PROMPT TEMPLATE
        template = """Answer the question using ONLY the context below. Reply in plain text. Do NOT use JSON, markdown, code blocks, bullet points, or numbered lists. Do NOT provide multiple options or alternatives. Give ONE direct, concise answer as a short paragraph. If the answer is not in the context, say: "I don't know the answer to that question."

Context: {context}

Question: {question}

Answer:"""

        self.prompt = PromptTemplate.from_template(template)

        # Setup retrieval chain
        retriever = self.vector_manager.get_retriever()
        if retriever:
            self.retrieval_chain = RunnableParallel({
                "context": RunnableLambda(lambda x: self._format_context_with_validation(retriever.invoke(x["question"]))),
                "question": RunnableLambda(lambda x: x["question"]),
            })

            self.query_chain = (
                self.retrieval_chain
                | self.prompt
                | self.model
                | RunnableLambda(self._robust_response_filter)
            )
        else:
            self.query_chain = None

  def _validate_question(self, question: str) -> str:
        """Validate and clean the question"""
        question = question.strip()

        # Check for empty or invalid questions
        if not question or len(question.strip(".,;!?")) < 2:
            return "INVALID_QUESTION"

        # Check for obvious prompt injection patterns
        injection_patterns = [
            "ignore previous instructions",
            "you are now",
            "forget everything",
            "system prompt",
            "act as",
            "pretend to be",
            "roleplay",
            "simulate",
        ]

        question_lower = question.lower()
        for pattern in injection_patterns:
            if pattern in question_lower:
                return "PROMPT_INJECTION_DETECTED"

        return question

  def _format_context_with_validation(self, docs: List[Document]) -> str:
        """Format retrieved documents as context with validation"""
        if not docs:
            return "EMPTY_CONTEXT"

        context = "\n\n".join([doc.page_content for doc in docs if doc.page_content.strip()])

        if not context.strip():
            return "EMPTY_CONTEXT"

        return context

  def _robust_response_filter(self, response: str) -> str:
        """Robust response filter with security checks"""
        response = response.strip()

        if not response:
            return "I don't know the answer to that question."

        # Handle special cases first
        if "INVALID_QUESTION" in response or "EMPTY_CONTEXT" in response:
            return "Please ask a specific question about the context."

        if "PROMPT_INJECTION_DETECTED" in response:
            return "I don't know the answer to that question."

        # Remove redundant prefixes but keep the response natural
        prefixes_to_remove = [
            "Based on the context, ",
            "According to the context, ",
            "The context states that ",
            "From the provided context, ",
            "The document mentions that ",
        ]

        response_lower = response.lower()
        for prefix in prefixes_to_remove:
            if response_lower.startswith(prefix.lower()):
                response = response[len(prefix):].strip()
                break

        # Security check - if response contains obvious external knowledge indicators
        external_knowledge_indicators = [
            "i am an ai",
            "i am a language model",
            "i am claude",
            "i am chatgpt",
            "as an ai assistant",
            "i don't have personal",
        ]

        response_lower = response.lower()
        for indicator in external_knowledge_indicators:
            if indicator in response_lower:
                return "I don't know the answer to that question."

        return response

  def _check_cache_for_similar_question(self, question: str) -> str:
        """Check if we have a cached answer for similar question"""
        if not self.conversation_cache:
            return None

        try:
            # Simple similarity check based on keywords
            question_words = set(question.lower().split())

            for cached_item in self.conversation_cache:
                cached_words = set(cached_item['question'].lower().split())

                # Calculate Jaccard similarity
                intersection = len(question_words.intersection(cached_words))
                union = len(question_words.union(cached_words))

                if union > 0:
                    similarity = intersection / union
                    if similarity >= self.cache_similarity_threshold:
                        print("(Using cached answer for similar question)")
                        return cached_item['answer']

            return None

        except Exception as e:
            logging.error(f"Error checking cache: {str(e)}")
            return None

  def _add_to_cache(self, question: str, answer: str):
      """Add question-answer pair to cache"""
      try:
          # Keep only last 10 conversations to prevent memory issues
          if len(self.conversation_cache) >= 10:
              self.conversation_cache.pop(0)

          self.conversation_cache.append({
              'question': question,
              'answer': answer,
              'timestamp': datetime.now().isoformat()
          })

      except Exception as e:
          logging.error(f"Error adding to cache: {str(e)}")

  def process_existing_files(self):
    """Process all existing PDF files"""
    pdf_files = list(Path(self.config.pdf_folder).glob("*.pdf"))

    if not pdf_files:
      print("No PDF files found in the documents folder")
      logging.info("No PDF files found in the documents folder")
      return

    print(f"Found {len(pdf_files)} PDF files to process")
    logging.info(f"Found {len(pdf_files)} PDF files to process")

    for pdf_file in pdf_files:
      self.process_single_file(str(pdf_file))

  def process_single_file(self, file_path: str):
    """Process a single PDF file"""
    try:
      file_hash = self.doc_processor._get_file_hash(file_path)

      if not self.vector_manager.needs_processing(file_path, file_hash):
        print(f"Skipping {os.path.basename(file_path)} - already processed")
        logging.info(f"Skipping {file_path} - already processed")
        return

      print(f"Processing {os.path.basename(file_path)}...")
      logging.info(f"Processing {file_path}")

      documents = self.doc_processor.process_pdf(file_path)

      if documents:
        self.vector_manager.add_documents(documents, file_path, file_hash)
        # Refresh query chain with updated vector store
        self.setup_query_chain()
        print(f"Successfully processed {os.path.basename(file_path)}")
        logging.info(f"Successfully processed {file_path}")
      else:
        print(f"No documents extracted from {os.path.basename(file_path)}")
        logging.warning(f"No documents extracted from {file_path}")

    except Exception as e:
      print(f"Error processing {os.path.basename(file_path)}: {str(e)}")
      logging.error(f"Error processing {file_path}: {str(e)}")

  def start_monitoring(self):
    """Start monitoring the PDF folder for new files"""
    if self.observer:
      self.stop_monitoring()

    event_handler = PDFFileHandler(self)
    self.observer = Observer()
    self.observer.schedule(event_handler, self.config.pdf_folder, recursive=False)
    self.observer.start()
    print(f"Started monitoring {self.config.pdf_folder} for new PDFs")
    logging.info(f"Started monitoring {self.config.pdf_folder} for new PDFs")

  def stop_monitoring(self):
    """Stop monitoring the PDF folder"""
    if self.observer:
      self.observer.stop()
      self.observer.join()
      self.observer = None
      print("Stopped monitoring for new PDFs")
      logging.info("Stopped monitoring for new PDFs")

  def query(self, question: str) -> str:
        """Query the RAG system with caching (non-streaming)"""
        if not self.query_chain:
            return "No documents have been processed yet. Please add PDF files to the documents folder."

        # Check cache first
        cached_answer = self._check_cache_for_similar_question(question)
        if cached_answer:
            return cached_answer

        try:
            response = self.query_chain.invoke({"question": question})

            # Add to cache if it's a valid response
            if response and response != "I don't know the answer to that question.":
                self._add_to_cache(question, response)

            return response
        except Exception as e:
            logging.error(f"Error processing query: {str(e)}")
            return "An error occurred while processing your question."

  def stream_query(self, question: str) -> Generator[str, None, None]:
        """Query the RAG system with streaming response and caching"""
        if not self.query_chain:
            yield "No documents have been processed yet. Please add PDF files to the documents folder."
            return

        # Check cache first
        cached_answer = self._check_cache_for_similar_question(question)
        if cached_answer:
            for char in cached_answer:
                yield char
            yield "\n"
            return

        try:
            # Validate question first
            validated_question = self._validate_question(question)

            if validated_question in ["INVALID_QUESTION", "PROMPT_INJECTION_DETECTED"]:
                if validated_question == "INVALID_QUESTION":
                    response = "Please ask a specific question about the context."
                else:
                    response = "I don't know the answer to that question."

                for char in response:
                    yield char
                yield "\n"
                return

            # Get context
            retriever = self.vector_manager.get_retriever()
            if not retriever:
                yield "No documents available for querying."
                return

            # Retrieve and validate context
            docs = retriever.invoke(question)
            context = self._format_context_with_validation(docs)

            if context == "EMPTY_CONTEXT":
                response = "Please ask a specific question about the context."
                for char in response:
                    yield char
                yield "\n"
                return

            # Format the prompt
            prompt_text = self.prompt.format(context=context, question=question)

            # Stream the response
            response_parts = []
            for chunk in self.model.stream(prompt_text):
                response_parts.append(chunk)
                yield chunk

            # Apply post-processing and caching
            complete_response = "".join(response_parts)
            filtered_response = self._robust_response_filter(complete_response)

            # Add to cache if it's a valid response
            if filtered_response and filtered_response != "I don't know the answer to that question.":
                self._add_to_cache(question, filtered_response)

            # If the filtered response is different, clear and send the filtered version
            if filtered_response != complete_response:
                yield "\r" + " " * len(complete_response) + "\r"  # Clear the line
                yield filtered_response

            yield "\n"

        except Exception as e:
            logging.error(f"Error processing streaming query: {str(e)}")
            yield "An error occurred while processing your question."


  def get_system_info(self) -> Dict[str, Any]:
    """Get system information"""
    info = {
      "pdf_folder": self.config.pdf_folder,
      "vector_db_path": self.config.vector_db_path,
      "processed_files": len(self.vector_manager.processed_files),
      "model": self.config.model_name,
      "embedding_model": self.config.embedding_model,
      "monitoring_active": self.observer is not None,
      "vector_store_ready": self.vector_manager.vector_store is not None,
      "debug_mode": self.config.debug_mode,
      "ocr_available": self.doc_processor.ocr_available,
      "ocr_dpi": self.config.ocr_dpi,
      "ocr_language": self.config.ocr_lang
    }

    # Add extraction method statistics
    if self.vector_manager.processed_files:
      extraction_stats = {}
      for file_info in self.vector_manager.processed_files.values():
        for method in file_info.get('extraction_methods', []):
          extraction_stats[method] = extraction_stats.get(method, 0) + 1
      info["extraction_methods_used"] = extraction_stats

    return info

  def toggle_debug_mode(self):
    """Toggle debug mode on/off"""
    self.config.debug_mode = not self.config.debug_mode
    self.doc_processor.config.debug_mode = self.config.debug_mode
    print(f"Debug mode {'enabled' if self.config.debug_mode else 'disabled'}")
    return self.config.debug_mode

  def get_extraction_summary(self) -> Dict[str, Any]:
    """Get detailed extraction summary"""
    summary = {
      "total_files": len(self.vector_manager.processed_files),
      "extraction_methods": {},
      "files_by_method": {},
      "total_chunks": 0
    }

    for file_path, file_info in self.vector_manager.processed_files.items():
      file_name = os.path.basename(file_path)
      methods = file_info.get('extraction_methods', ['unknown'])
      chunks = file_info.get('num_chunks', 0)

      summary["total_chunks"] += chunks

      for method in methods:
        if method not in summary["extraction_methods"]:
          summary["extraction_methods"][method] = {"files": 0, "chunks": 0}
          summary["files_by_method"][method] = []

        summary["extraction_methods"][method]["files"] += 1
        summary["extraction_methods"][method]["chunks"] += chunks
        summary["files_by_method"][method].append({
          "file": file_name,
          "chunks": chunks,
          "processed_at": file_info.get('processed_at', 'unknown')
        })

    return summary

def main():
  """Main function to run the RAG system"""
  print("Initializing Robust RAG System with OCR Support...")

  # Create custom config
  config = RAGConfig(
    pdf_folder="./documents",
    vector_db_path="./vector_db",
    model_name="gemma3:4b",
    embedding_model="bge-m3:latest",
    debug_mode=False,  # Set to True to see detailed extraction info
    ocr_dpi=300,       # Higher DPI for better OCR quality
    ocr_lang='eng'     # OCR language
  )

  # Initialize RAG system
  rag = RobustRAGSystem(config)

  # Start monitoring for new files
  rag.start_monitoring()

  # Print system info
  info = rag.get_system_info()
  print("\nSystem Information:")
  for key, value in info.items():
    print(f"  {key}: {value}")

  # Print extraction summary
  extraction_summary = rag.get_extraction_summary()
  print("\nExtraction Summary:")
  print(f"  Total files processed: {extraction_summary['total_files']}")
  print(f"  Total chunks created: {extraction_summary['total_chunks']}")

  if extraction_summary['extraction_methods']:
    print("  Extraction methods used:")
    for method, stats in extraction_summary['extraction_methods'].items():
      print(f"    {method}: {stats['files']} files, {stats['chunks']} chunks")

  print(f"\nDrop PDF files into: {config.pdf_folder}")
  print("Ready to answer questions!")
  print("Type 'debug' to toggle debug mode")
  print("Type 'info' to show system information")
  print("Type 'summary' to show extraction summary")
  print("Type 'help' for available commands\n")

  # Interactive query loop with streaming
  try:
    while True:
      user_input = input("\nAsk a question (or 'quit' to exit): ").strip()

      if user_input.lower() in ['quit', 'exit', 'q']:
        break

      if not user_input:
        continue

      # Handle special commands
      if user_input.lower() == 'debug':
        debug_status = rag.toggle_debug_mode()
        print(f"Debug mode is now {'ON' if debug_status else 'OFF'}")
        continue

      if user_input.lower() == 'info':
        info = rag.get_system_info()
        print("\nCurrent System Information:")
        for key, value in info.items():
          print(f"  {key}: {value}")
        continue

      if user_input.lower() == 'summary':
        summary = rag.get_extraction_summary()
        print("\nExtraction Summary:")
        print(f"  Total files: {summary['total_files']}")
        print(f"  Total chunks: {summary['total_chunks']}")

        if summary['extraction_methods']:
          print("  Methods used:")
          for method, stats in summary['extraction_methods'].items():
            print(f"    {method}: {stats['files']} files, {stats['chunks']} chunks")

        if summary['files_by_method']:
          print("  Files by method:")
          for method, files in summary['files_by_method'].items():
            print(f"    {method}:")
            for file_info in files:
              print(f"      - {file_info['file']}: {file_info['chunks']} chunks")
        continue

      if user_input.lower() == 'help':
        print("\nAvailable Commands:")
        print("  debug    - Toggle debug mode (shows detailed extraction info)")
        print("  info     - Show system information")
        print("  summary  - Show extraction summary")
        print("  help     - Show this help message")
        print("  quit     - Exit the program")
        print("  Or ask any question about your documents!")
        continue

      print("Searching...\n", end=" ")

      # Stream the response
      for chunk in rag.stream_query(user_input):
        print(chunk, end="", flush=True)

      print()  # New line after streaming

  except KeyboardInterrupt:
    print("\n\nShutting down...")

  finally:
    rag.stop_monitoring()
    print("RAG system stopped.")

if __name__ == "__main__":
  main()